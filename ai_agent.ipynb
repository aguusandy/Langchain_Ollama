{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "AI Agent Chatbot\n",
    "    - Langchain Tutos: https://python.langchain.com/docs/tutorials/chatbot/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2589d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDatabaseTool\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f57655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "# model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"llama3.2\", model_provider=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "089590b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f3fc631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a kind artificial assistant. You are a helpful assistant. You try to respond to every question with helpful information. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ca7d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"English\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76525320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# # Async function for node:\n",
    "# async def call_model(state: MessagesState):\n",
    "#     response = await model.ainvoke(state[\"messages\"])\n",
    "#     return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f29757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x25cbd0d8860>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the (single) node in the graph\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# async \n",
    "# workflow = StateGraph(state_schema=MessagesState)\n",
    "# workflow.add_edge(START, \"model\")\n",
    "# workflow.add_node(\"model\", call_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7b0765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "# async\n",
    "# app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44b6d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = \"abc123\" # thread of the conversation\n",
    "config = {\"configurable\": {\"thread_id\": thread }} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c899db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I think I see a pattern here, Bob! Do you want to talk about something in particular, or would you like me to try and guess what's on your mind?\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Bob.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Async invocation:\n",
    "# output = await app.ainvoke({\"messages\": input_messages, \"language\": language}, config)\n",
    "# output[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_rag_langchain_llama)",
   "language": "python",
   "name": "venv_rag_langchain_llama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
